{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This notebook is authored by Shashwat Bhardwaj'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pucho Deep Learning Intern Assignment\n",
    "<br>\n",
    "Shashwat Bhardwaj<br>\n",
    "B.Tech : Computer Science and Engineering<br>\n",
    "VI Semester<br>\n",
    "Mahraja Agrasen Institute of Technology<br>\n",
    "New Delhi, India<br><br><br><br>\n",
    "\n",
    "\n",
    "PS: I have also attached my resume with this notebook<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment & Semantic Analysis on IMDB Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is aimed at using Recurrent Neural Network architectures like GRU(s): Gated Recurrent Units and LSTM(s): Long Short Term Memory which is a special case of GRU, to contrast the working & accuracies between the two and to project their working domains. This is a kaggle challenge Bag of Words Meets Bags of Popcorn. Basic goal of the challenge is to predict sentiments from the given reviews about movies. The dataset from IMDB is used for training of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries used in this notebook\n",
    "\n",
    "###### General Purpose Libraries: <br>\n",
    "\n",
    "- numpy: for Linear Algebra\n",
    "- pandas: for csv loading of files\n",
    "- bs4: Beautiful soap for scrapping related tasks\n",
    "- pickle: used for serializing and de-serializing a Python object structure. Any object in Python can be pickled so that it can be saved on disk.\n",
    "- os: used for directory related and os related tasks\n",
    "- re: used for incorporating regular expressions in python\n",
    "- lxml: used as a support for bs4 to work. It's a xml parser, used for parsing XML pages\n",
    "- html: used as a support for bs4 to work. It's a html parser, used for parsing HTML pages\n",
    "- matplotlib: used for plotting graphs\n",
    "- sklearn: for model evaluation tasks\n",
    "<br>\n",
    "\n",
    "###### Special purpose and deep learning libraries:<br>\n",
    "\n",
    "- nltk: Natural Language Toolkit. Used for NLP related tasks like stemming etc.\n",
    "- keras: A high level API of tensorflow used for fast development of neural network and deep neural model architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps used:  <br>\n",
    "- Importing libraries\n",
    "- Loading data\n",
    "- Exploring the data\n",
    "- Defining preprocessing functions\n",
    "- Data-Preprocessing step\n",
    "- Generaing text sequence for RNN model\n",
    "- Defining RNN model\n",
    "- Implementing GRU\n",
    "- Implementing LSTM\n",
    "- Evaluation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries <br>\n",
    "We import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re\n",
    "from nltk.corpus import stopwords\n",
    "import lxml\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "We load the training and the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"labeledTrainData.tsv\", header = 0, delimiter = '\\t')\n",
    "test = pd.read_csv(\"testData.tsv\", header = 0, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "In this section we explore our dataset and gain useful insights from it. This is a very crucial step in any deep learning project and action. As all machine and deep learning is about data, the primary and the most crucial part of any ML and DL development is having a strong knowledge of the data to be operated on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      "id           25000 non-null object\n",
      "sentiment    25000 non-null int64\n",
      "review       25000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can infer from the above data that:\n",
    "- It has 25000 entries\n",
    "- It has 3 columns named id, sentiment and review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see first ten rows of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8196_8</td>\n",
       "      <td>1</td>\n",
       "      <td>I dont know why people think this is such a ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7166_2</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie could have been very good, but come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10633_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I watched this video at a friend's house. I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>319_1</td>\n",
       "      <td>0</td>\n",
       "      <td>A friend of mine bought this film for £1, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8713_10</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;This movie is full of references. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sentiment                                             review\n",
       "0   5814_8          1  With all this stuff going down at the moment w...\n",
       "1   2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2   7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3   3630_4          0  It must be assumed that those who praised this...\n",
       "4   9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
       "5   8196_8          1  I dont know why people think this is such a ba...\n",
       "6   7166_2          0  This movie could have been very good, but come...\n",
       "7  10633_1          0  I watched this video at a friend's house. I'm ...\n",
       "8    319_1          0  A friend of mine bought this film for £1, and ...\n",
       "9  8713_10          1  <br /><br />This movie is full of references. ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see read a few reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter. \n",
      "\n",
      "I would love to have that two hours of my life back. It seemed to be several clips from Steve's Animal Planet series that was spliced into a loosely constructed script. Don't Go, If you must see it, wait for the video ... \n",
      "\n",
      "Kurosawa is a proved humanitarian. This movie is totally about people living in poverty. You will see nothing but angry in this movie. It makes you feel bad but still worth. All those who's too comfortable with materialization should spend 2.5 hours with this movie.\n"
     ]
    }
   ],
   "source": [
    "# Seeing the review\n",
    "print(train['review'][0],'\\n')\n",
    "print(train['review'][23],'\\n')\n",
    "print(train['review'][456])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the length and datatype of these reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2302\n",
      "221\n",
      "266\n"
     ]
    }
   ],
   "source": [
    "print(len(train['review'][0]))\n",
    "print(len(train['review'][23]))\n",
    "print(len(train['review'][456]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train['review'][0]))\n",
    "print(type(train['review'][23]))\n",
    "print(type(train['review'][456]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These reviews are of type String"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do a little preprocessing first. For the sentiment column, we map it with 1 if after seperating words which have ' _ ' in between , if their length greater than 5, basically a dummy review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=\"My name is Tony_Stark and I am the greatest_avenger earth has_ever_seen\"\n",
    "# ans=x.strip('\"').split(\"_\")\n",
    "# print(ans)\n",
    "# print(ans[1])\n",
    "# b= lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0\n",
    "# print(b)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      "id           25000 non-null object\n",
      "sentiment    25000 non-null int64\n",
      "review       25000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now basically we are creating a final dataset which can be fed to the neural network. So we create some preprocessing helper functions for us. These functions basically help in extracting out the real and actual fleshy content from the dataset, by usage of regular expressions and all to eliminate all the irrevelant data like html tags and all.\n",
    "\n",
    "Basic Preprocessing Helper functions used are:\n",
    "\n",
    "- html_to_text: This function converts a html document or an xml file to text by eliminating all html tags and xml tags. For this we use BeautifulSoup library.\n",
    "\n",
    "- letters_only: This function returns the inpit strings as just pieces of plain texts without any puntuation marks or numbers\n",
    "\n",
    "- rnn_tokenizer_review_preprocess: This function is used before fitting/transforming RNN tokenizer to convert Html to text and remove punctuations\n",
    "\n",
    "- get_train_val_data: Extracts features (using reviews_to_features_fn), splits into train/test data, and returns x_train, y_train, x_test, y_test.  If no feature extraction function is provided, x_train/x_test will simply consist of a Series of all the reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_text(review):\n",
    "    \"\"\"Return extracted text string from provided HTML string.\"\"\"\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    if len(review_text) == 0:\n",
    "        review_text = review\n",
    "    review_text = re.sub(r\"\\<.*\\>\", \"\", review_text)\n",
    "    try:\n",
    "        review_text = review_text.encode('ascii', 'ignore').decode('ascii')#ignore \\xc3 etc.\n",
    "    except UnicodeDecodeError:\n",
    "        review_text = review_text.decode(\"ascii\", \"ignore\")\n",
    "    return review_text\n",
    "\n",
    "\n",
    "def letters_only(text):\n",
    "    \"\"\"Return input string with only letters (no punctuation, no numbers).\"\"\"\n",
    "    # It is probably worth experimenting with milder prepreocessing (eg just removing punctuation)\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "def rnn_tokenizer_review_preprocess(review):\n",
    "    \"\"\"Preprocessing used before fitting/transforming RNN tokenizer - Html->text, remove punctuation/#s, lowercase.\"\"\"\n",
    "    return letters_only(html_to_text(review)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_data(reviews_to_features_fn=None, df = train):\n",
    "    \"\"\"Extracts features (using reviews_to_features_fn), splits into train/test data, and returns\n",
    "    x_train, y_train, x_test, y_test.  If no feature extraction function is provided, x_train/x_test will\n",
    "    simply consist of a Series of all the reviews.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = 1000\n",
    "    # Shuffle data frame rows\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    # We also ensure permuations randomly !!\n",
    "    \n",
    "    print(df)\n",
    "    df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "    if reviews_to_features_fn:\n",
    "        feature_rows = df[\"review\"].map(reviews_to_features_fn)\n",
    "        if type(feature_rows[0]) == np.ndarray:\n",
    "            num_instances = len(feature_rows)\n",
    "            num_features = len(feature_rows[0])\n",
    "            x = np.concatenate(feature_rows.values).reshape((num_instances, num_features))\n",
    "        else:\n",
    "            x = feature_rows\n",
    "    else:\n",
    "        x = df[\"review\"]\n",
    "\n",
    "    y = df[\"sentiment\"]\n",
    "\n",
    "    # Split 80/20\n",
    "    test_start_index = int(df.shape[0] * .8)\n",
    "    x_train = x[0:test_start_index]\n",
    "    y_train = y[0:test_start_index]\n",
    "    x_val = x[test_start_index:]\n",
    "    y_val = y[test_start_index:]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  sentiment                                             review\n",
      "0       5814_8          1  With all this stuff going down at the moment w...\n",
      "1       2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2       7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3       3630_4          0  It must be assumed that those who praised this...\n",
      "4       9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
      "5       8196_8          1  I dont know why people think this is such a ba...\n",
      "6       7166_2          0  This movie could have been very good, but come...\n",
      "7      10633_1          0  I watched this video at a friend's house. I'm ...\n",
      "8        319_1          0  A friend of mine bought this film for £1, and ...\n",
      "9      8713_10          1  <br /><br />This movie is full of references. ...\n",
      "10      2486_3          0  What happens when an army of wetbacks, towelhe...\n",
      "11     6811_10          1  Although I generally do not like remakes belie...\n",
      "12     11744_9          1  \\Mr. Harvey Lights a Candle\\\" is anchored by a...\n",
      "13      7369_1          0  I had a feeling that after \\Submerged\\\", this ...\n",
      "14     12081_1          0  note to George Litman, and others: the Mystery...\n",
      "15      3561_4          0  Stephen King adaptation (scripted by King hims...\n",
      "16      4489_1          0  `The Matrix' was an exciting summer blockbuste...\n",
      "17      3951_2          0  Ulli Lommel's 1980 film 'The Boogey Man' is no...\n",
      "18     3304_10          1  This movie is one among the very few Indian mo...\n",
      "19     9352_10          1  Most people, especially young people, may not ...\n",
      "20      3374_7          1  \\Soylent Green\\\" is one of the best and most d...\n",
      "21     10782_7          1  Michael Stearns plays Mike, a sexually frustra...\n",
      "22     5414_10          1  This happy-go-luck 1939 military swashbuckler,...\n",
      "23     10492_1          0  I would love to have that two hours of my life...\n",
      "24      3350_3          0  The script for this movie was probably found i...\n",
      "25      6581_7          1  Looking for Quo Vadis at my local video store,...\n",
      "26      2203_3          0  Note to all mad scientists everywhere: if you'...\n",
      "27       689_1          0  What the ........... is this ? This must, with...\n",
      "28      9152_1          0  Intrigued by the synopsis (every gay video the...\n",
      "29      6077_1          0  Would anyone really watch this RUBBISH if it d...\n",
      "...        ...        ...                                                ...\n",
      "24970   9389_7          1  Red Rock West (1993)<br /><br />Nicolas Cage g...\n",
      "24971   9251_9          1  what can i say?, ms Erika Eleniak is my favori...\n",
      "24972  1422_10          1  The spoiler warning is for those people who wa...\n",
      "24973   7415_2          0  What do you call a horror story without horror...\n",
      "24974   7492_7          1  Though not a horror film in the traditional se...\n",
      "24975  7689_10          1  This was what black society was like before th...\n",
      "24976  12370_4          0  They probably should have called this movie Th...\n",
      "24977   5625_8          1  Attractive Marjorie(Farrah Fawcett)lives in fe...\n",
      "24978   9397_9          1  Vaguely reminiscent of great 1940's westerns, ...\n",
      "24979   5992_7          1  I admit I had no idea what to expect before vi...\n",
      "24980  2488_10          1  To me, the final scene, in which Harris respon...\n",
      "24981  9627_10          1  This is by far the funniest short made by the ...\n",
      "24982   3822_2          0  To be a Buster Keaton fan is to have your hear...\n",
      "24983   5983_4          0  I was one of those \\few Americans\\\" that grew ...\n",
      "24984   8021_2          0  Visually disjointed and full of itself, the di...\n",
      "24985   3471_3          0  this movie had more holes than a piece of swis...\n",
      "24986  6034_10          1  Last November, I had a chance to see this film...\n",
      "24987   1988_9          1  First off, I'd like to make a correction on an...\n",
      "24988   7623_9          1  While originally reluctant to jump on the band...\n",
      "24989   5974_7          1  I heard about this movie when watching VH1's \\...\n",
      "24990   2034_9          1  I've never been huge on IMAX films. They're co...\n",
      "24991   9416_3          0  Steve McQueen has certainly a lot of loyal fan...\n",
      "24992  10994_1          0  Sometimes you wonder how some people get fundi...\n",
      "24993  10957_3          0  I am a student of film, and have been for seve...\n",
      "24994   2372_1          0  Unimaginably stupid, redundant and humiliating...\n",
      "24995   3453_3          0  It seems like more consideration has gone into...\n",
      "24996   5064_1          0  I don't believe they made this film. Completel...\n",
      "24997  10905_3          0  Guy is a loser. Can't get girls, needs to buil...\n",
      "24998  10194_3          0  This 30 minute documentary Buñuel made in the ...\n",
      "24999   8478_8          1  I saw this movie as a child and it broke my he...\n",
      "\n",
      "[25000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val = get_train_val_data(rnn_tokenizer_review_preprocess)\n",
    "x_test = test[\"review\"].map(rnn_tokenizer_review_preprocess)\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our final X-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   41,   28, 2777],\n",
       "       [   0,    0,    0, ..., 3088,  128,   56],\n",
       "       [   0,    0,    0, ...,    3,  664, 2406],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   21,  121,    7],\n",
       "       [   0,    0,    0, ...,   10,   13,   29],\n",
       "       [   0,    0,    0, ...,    3,   49,   19]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the text sequence for RNN model\n",
    "\n",
    "We now generate the text sequence which is to be fed to the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a seed to the random generator, so that it generates same values everytime we execute the code. We limit the most frequently used words and maximum review length to:\n",
    "\n",
    "- Prevent overfitting\n",
    "- Save computation time by reducing uncessary text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "num_most_freq_words_to_include = 5000\n",
    "MAX_REVIEW_LENGTH_FOR_KERAS_RNN = 500\n",
    "embedding_vector_length = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also convert everything into a list for easy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_review_list = x_train.tolist()\n",
    "val_review_list = x_val.tolist()\n",
    "test_review_list = x_test.tolist()\n",
    "all_review_list = x_train.tolist() + x_val.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer: This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_most_freq_words_to_include)\n",
    "tokenizer.fit_on_texts(all_review_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more steps: <br>\n",
    "\n",
    "- We must ensure all sequences are of same length\n",
    "- conversion of texts to numbers\n",
    "\n",
    "    - pad_sequences: is used to ensure that all sequences in a list have the same length. \n",
    "    - tokenizer.texts_to_sequences: Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews_tokenized = tokenizer.texts_to_sequences(train_review_list)\n",
    "x_train = pad_sequences(train_reviews_tokenized, maxlen=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\n",
    "val_review_tokenized = tokenizer.texts_to_sequences(val_review_list)\n",
    "x_val = pad_sequences(val_review_tokenized, maxlen=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\n",
    "test_review_tokenized = tokenizer.texts_to_sequences(test_review_list)\n",
    "x_test = pad_sequences(test_review_tokenized, maxlen=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Architecture <br>\n",
    "\n",
    "Its the time to define our RNN Architecture <br>\n",
    "\n",
    "\n",
    "Recurrent Neural Network(RNN) is a type of Neural Network where the output from previous step are fed as input to the current step. In traditional neural networks, all the inputs and outputs are independent of each other, but in cases like when it is required to predict the next word of a sentence, the previous words are required and hence there is a need to remember the previous words. Thus RNN came into existence, which solved this issue with the help of a Hidden Layer. The main and most important feature of RNN is Hidden state, which remembers some information about a sequence. RNNs have a “memory” which remember all information about what has been calculated. It uses the same parameters for each input as it performs the same task on all the inputs or hidden layers to produce the output. This reduces the complexity of parameters, unlike other neural networks.RNNs have many types but the most used are:\n",
    "\n",
    "GRU: Gated Recurrent Unit\n",
    "\n",
    "LSTM: Long Short Term Memory<br>\n",
    "\n",
    "#### Training through RNN\n",
    "\n",
    "- A single time step of the input is provided to the network.\n",
    "- Then calculate its current state using set of current input and the previous state.\n",
    "- The current ht becomes ht-1 for the next time step.\n",
    "- One can go as many time steps according to the problem and join the information from all the previous states.\n",
    "- Once all the time steps are completed the final current state is used to calculate the output.\n",
    "- The output is then compared to the actual output i.e the target output and the error is generated.\n",
    "- The error is then back-propagated to the network to update the weights and hence the network (RNN) is trained.<br>\n",
    "\n",
    "#### Advantages of Recurrent Neural Network\n",
    "- An RNN remembers each and every information through time. It is useful in time series prediction only because of the feature to remember previous inputs as well. This is called Long Short Term Memory.\n",
    "- Recurrent neural network are even used with convolutional layers to extend the effective pixel neighborhood.<br>\n",
    "\n",
    "#### Disadvantages of Recurrent Neural Network\n",
    "\n",
    "- Gradient vanishing and exploding problems, solved by GRUs and LSTMs\n",
    "- Training an RNN is a very difficult task.\n",
    "- It cannot process very long sequences if using tanh or relu as an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dropout, Conv1D, MaxPool1D, GRU, LSTM, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architectures we've used for GRU and LSTM is:\n",
    "    \n",
    "    - Embedding layer has max_review_length x most_freq_length_to_include dimension\n",
    "    - Has 100 units \n",
    "    - Adam optimizer\n",
    "    - Loss function used is Binary Cross Entropy (summation(0,m)[-ylogy_ +(1-y)log(1-y_)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(use_cnn = True, use_lstm = False):\n",
    "    input_sequences = Input(shape = (MAX_REVIEW_LENGTH_FOR_KERAS_RNN,))\n",
    "    initial_dropout = 0.2\n",
    "    embedding_layer = Embedding(input_dim = num_most_freq_words_to_include, \n",
    "                                output_dim = embedding_vector_length,\n",
    "                                input_length = MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\n",
    "    X = embedding_layer(input_sequences)\n",
    "    X = Dropout(0.2)(X)\n",
    "    if use_cnn:\n",
    "        X = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(X)\n",
    "        X = MaxPool1D(pool_size=2)(X)\n",
    "        \n",
    "    # Add GRU layers\n",
    "    dropout_W = 0.0\n",
    "    dropout_U = 0.0\n",
    "    \n",
    "    if use_lstm:\n",
    "        X = LSTM(100, dropout = dropout_W, recurrent_dropout = dropout_U)(X)\n",
    "    else:\n",
    "        X = GRU(100, dropout=dropout_W, recurrent_dropout=dropout_U)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    outputs= Dense(1, activation='sigmoid')(X)\n",
    "    model = Model(inputs = input_sequences, outputs = outputs)\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs have many types but the most used are:\n",
    "\n",
    "- GRU: Gated Recurrent Unit\n",
    "\n",
    "- LSTM: Long Short Term Memory\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with forget gate but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling and speech signal modeling was found to be similar to that of LSTM. GRUs have been shown to exhibit even better performance on certain smaller datasets.\n",
    "GRUs were aimed to solve vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intialization of gru model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shashwat bhardwaj\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\shashwat bhardwaj\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "gru_model = rnn_model(use_lstm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the final GRU constructed before training of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               39900     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 203,105\n",
      "Trainable params: 203,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 203,105 parameters for the network to be trained! These is a humongous set of parameters. But we have a Nvidia GTX 1060 GPU, which would enable fast computation. So now, its time to train our network. We are using 3 epochs here for training, keeping batch size as 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shashwat bhardwaj\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 404s 20ms/step - loss: 0.5030 - acc: 0.7262 - val_loss: 0.3105 - val_acc: 0.8748\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 1129s 56ms/step - loss: 0.2732 - acc: 0.8925 - val_loss: 0.3013 - val_acc: 0.8822\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 369s 18ms/step - loss: 0.2125 - acc: 0.9194 - val_loss: 0.2851 - val_acc: 0.8884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21b27612b00>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model.fit(x_train, y_train, batch_size=64, epochs=3, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since we have our model trained, lets have predictions on the test set, to get final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_gru = gru_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections that make it a \"general purpose computer\" (that is, it can compute anything that a Turing machine can). It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. Bloomberg Business Week wrote: \"These powers make LSTM arguably the most commercial AI achievement, used for everything from predicting diseases to composing music.\"\n",
    "<br>\n",
    "A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n",
    "<br>\n",
    "LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the exploding and vanishing gradient problems that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intialization of LSTM model and its final construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = rnn_model(use_lstm=True)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its training time for our model this time using LSTM over 3 epochs, keeping batch size as 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 162s 8ms/step - loss: 0.4683 - acc: 0.7600 - val_loss: 0.3051 - val_acc: 0.8832\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 119s 6ms/step - loss: 0.2687 - acc: 0.8934 - val_loss: 0.3002 - val_acc: 0.8764\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 116s 6ms/step - loss: 0.2185 - acc: 0.9160 - val_loss: 0.3141 - val_acc: 0.8890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21cb78a6a20>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(x_train, y_train, batch_size = 64, epochs = 3, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since we have our model trained, lets have predictions on the test set, to get final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_lstm = lstm_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the accuracies of both the models on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC socre for GRU model is : 0.9489.\n",
      "The AUC socre for LSTM model is : 0.9457.\n"
     ]
    }
   ],
   "source": [
    "print(\"The AUC socre for GRU model is : %.4f.\" %roc_auc_score(y_test, y_test_pred_gru))\n",
    "print(\"The AUC socre for LSTM model is : %.4f.\" %roc_auc_score(y_test, y_test_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy on GRU Model is 94.89% \n",
    "- Accuracy on LSTM model is 94.57%\n",
    "\n",
    "Which are pretty good results !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_list = [y_test_pred_gru, y_test_pred_lstm]\n",
    "label_list = [\"GRU\", \"LSTM\"]\n",
    "pred_label = zip(y_pred_list, label_list)\n",
    "for y_pred, lbl in pred_label:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    plt.plot(fpr, tpr, label = lbl)\n",
    "\n",
    "plt.xlabel(\"True Postive Rate\")\n",
    "plt.ylabel(\"False Positive Rate\")\n",
    "plt.title(\"ROC Curve for RNN Models\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
